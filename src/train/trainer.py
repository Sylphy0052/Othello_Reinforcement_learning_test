"""
AlphaZero Trainer

å­¦ç¿’ãƒ«ãƒ¼ãƒ—å…¨ä½“ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:
1. Self-Play ã§ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
2. Replay Buffer ã«æ ¼ç´
3. ãƒŸãƒ‹ãƒãƒƒãƒã§å­¦ç¿’ï¼ˆAMPä½¿ç”¨ï¼‰
4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
5. TensorBoard ãƒ­ã‚®ãƒ³ã‚°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
from typing import Optional, List
from collections import deque
import time
import sys


class AlphaZeroTrainer:
    """
    AlphaZeroå­¦ç¿’ãƒ«ãƒ¼ãƒ—

    Self-Play â†’ Replay Buffer â†’ Train ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†
    """

    def __init__(
        self,
        model: nn.Module,
        device: torch.device,
        replay_buffer,
        self_play_worker,
        config: dict,
        checkpoint_dir: str = "data/models",
        log_dir: str = "data/logs",
    ):
        """
        Args:
            model: OthelloResNet
            device: torch.device
            replay_buffer: ReplayBuffer
            self_play_worker: SelfPlayWorker
            config: è¨­å®šè¾æ›¸
            checkpoint_dir: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å…ˆ
            log_dir: TensorBoard ãƒ­ã‚°ä¿å­˜å…ˆ
        """
        self.model = model
        self.device = device
        self.replay_buffer = replay_buffer
        self.self_play_worker = self_play_worker
        self.config = config

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.SGD(
            model.parameters(),
            lr=config.get("lr", 0.001),
            momentum=config.get("momentum", 0.9),
            weight_decay=config.get("weight_decay", 0.0001),
        )

        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=config.get("lr_step_size", 100),
            gamma=config.get("lr_gamma", 0.1),
        )

        # AMP Scalerï¼ˆæ··åˆç²¾åº¦å­¦ç¿’ï¼‰
        self.scaler = torch.amp.GradScaler('cuda') if device.type == "cuda" else None

        # TensorBoard Writer
        self.writer = SummaryWriter(log_dir=str(self.log_dir))

        # å­¦ç¿’çµ±è¨ˆ
        self.global_step = 0
        self.epoch = 0

        # é€²æ—è¿½è·¡ç”¨
        self.loss_history: deque = deque(maxlen=20)  # ç›´è¿‘20ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®Loss
        self.iter_times: deque = deque(maxlen=10)    # ç›´è¿‘10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ‰€è¦æ™‚é–“
        self.training_start_time: float = 0

    def _format_time(self, seconds: float) -> str:
        """ç§’æ•°ã‚’è¦‹ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            m, s = divmod(int(seconds), 60)
            return f"{m}m{s:02d}s"
        else:
            h, remainder = divmod(int(seconds), 3600)
            m, s = divmod(remainder, 60)
            return f"{h}h{m:02d}m"

    def _get_eta(self, current_iter: int, total_iters: int) -> str:
        """äºˆæƒ³æ®‹ã‚Šæ™‚é–“ã‚’è¨ˆç®—"""
        if len(self.iter_times) == 0:
            return "è¨ˆç®—ä¸­..."
        avg_time = sum(self.iter_times) / len(self.iter_times)
        remaining = total_iters - current_iter
        eta_seconds = avg_time * remaining
        return self._format_time(eta_seconds)

    def _get_loss_trend(self) -> str:
        """Lossã®å‚¾å‘ã‚’è¡¨ç¤º"""
        if len(self.loss_history) < 2:
            return ""
        recent = list(self.loss_history)[-5:]  # ç›´è¿‘5å€‹
        if len(recent) < 2:
            return ""
        diff = recent[-1] - recent[0]
        if diff < -0.05:
            return "â†“"
        elif diff > 0.05:
            return "â†‘"
        else:
            return "â†’"

    def _print_progress_bar(self, current: int, total: int, width: int = 30) -> str:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ç”Ÿæˆ"""
        pct = current / total
        filled = int(width * pct)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}] {pct*100:5.1f}%"

    def train(
        self,
        num_iterations: int,
        self_play_episodes_per_iter: int = 100,
        train_epochs_per_iter: int = 10,
        batch_size: int = 256,
        checkpoint_interval: int = 10,
    ):
        """
        å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ

        Args:
            num_iterations (int): å­¦ç¿’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
            self_play_episodes_per_iter (int): ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®Self-Playã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
            train_epochs_per_iter (int): ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
            checkpoint_interval (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        """
        print("=" * 70)
        print("AlphaZero Training Started")
        print("=" * 70)
        print(f"Device: {self.device}")
        print(f"Model: {self.model.__class__.__name__}")
        print(f"Batch Size: {batch_size}")
        print(f"Iterations: {num_iterations}")
        print("=" * 70)

        self.training_start_time = time.time()

        for iteration in range(1, num_iterations + 1):
            iter_start_time = time.time()

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ: é€²æ—ãƒãƒ¼ã¨äºˆæƒ³æ®‹ã‚Šæ™‚é–“
            progress_bar = self._print_progress_bar(iteration, num_iterations)
            eta = self._get_eta(iteration, num_iterations)
            elapsed = self._format_time(time.time() - self.training_start_time)

            print(f"\n{'â”€'*70}")
            print(f"Iter {iteration}/{num_iterations} {progress_bar}  çµŒé:{elapsed}  æ®‹ã‚Š:{eta}")
            print(f"{'â”€'*70}")

            # 1. Self-Play ã§ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            self_play_start = time.time()

            training_data = self.self_play_worker.execute_episodes(
                num_episodes=self_play_episodes_per_iter,
                add_dirichlet_noise=True,
            )

            self.replay_buffer.add(training_data)
            self_play_time = time.time() - self_play_start

            # 2. å­¦ç¿’
            avg_loss = 0.0
            train_time = 0.0
            if self.replay_buffer.is_ready(batch_size):
                train_start = time.time()

                avg_loss = self._train_epochs(
                    num_epochs=train_epochs_per_iter,
                    batch_size=batch_size,
                )

                train_time = time.time() - train_start
                self.loss_history.append(avg_loss)

                # TensorBoard ãƒ­ã‚®ãƒ³ã‚°
                self.writer.add_scalar("Loss/train", avg_loss, iteration)
                self.writer.add_scalar("Time/self_play", self_play_time, iteration)
                self.writer.add_scalar("Time/train", train_time, iteration)
                self.writer.add_scalar("Buffer/size", len(self.replay_buffer), iteration)

                # ãƒãƒƒãƒ•ã‚¡çµ±è¨ˆ
                buffer_stats = self.replay_buffer.get_statistics()
                self.writer.add_scalar("Buffer/value_mean", buffer_stats["value_mean"], iteration)
                self.writer.add_scalar("Buffer/value_std", buffer_stats["value_std"], iteration)

            # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“ã‚’è¨˜éŒ²
            iter_time = time.time() - iter_start_time
            self.iter_times.append(iter_time)

            # ã‚µãƒãƒªãƒ¼è¡Œ
            trend = self._get_loss_trend()
            print(f"  Loss: {avg_loss:.4f} {trend}  |  "
                  f"Buffer: {len(self.replay_buffer):,}  |  "
                  f"Self-Play: {self_play_time:.0f}s  |  "
                  f"Train: {train_time:.1f}s")

            # 3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if iteration % checkpoint_interval == 0:
                self.save_checkpoint(f"checkpoint_iter_{iteration}.pt")

                # å®šæœŸçš„ãªLossæ¨ç§»è¡¨ç¤º
                if len(self.loss_history) >= 5:
                    recent_losses = list(self.loss_history)[-5:]
                    loss_str = " â†’ ".join([f"{l:.3f}" for l in recent_losses])
                    print(f"  ğŸ“ˆ Lossæ¨ç§» (ç›´è¿‘5): {loss_str}")

        total_time = self._format_time(time.time() - self.training_start_time)
        print("\n" + "=" * 70)
        print(f"Training Completed! Total Time: {total_time}")
        print("=" * 70)

        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self.save_checkpoint("final_model.pt")
        self.writer.close()

    def _train_epochs(
        self,
        num_epochs: int,
        batch_size: int,
    ) -> float:
        """
        æŒ‡å®šã‚¨ãƒãƒƒã‚¯æ•°ã ã‘å­¦ç¿’

        Args:
            num_epochs (int): ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º

        Returns:
            float: å¹³å‡æå¤±
        """
        self.model.train()
        total_loss = 0.0
        total_batches = 0

        for epoch in range(num_epochs):
            # ãƒŸãƒ‹ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            states, target_policies, target_values = self.replay_buffer.sample(batch_size)

            # Tensor ã«å¤‰æ›
            states = torch.from_numpy(states).to(self.device)
            target_policies = torch.from_numpy(target_policies).to(self.device)
            target_values = torch.from_numpy(target_values).to(self.device)

            # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
            loss = self._train_step(states, target_policies, target_values)

            total_loss += loss
            total_batches += 1

            self.global_step += 1
            self.epoch += 1

        avg_loss = total_loss / total_batches
        return avg_loss

    def _train_step(
        self,
        states: torch.Tensor,
        target_policies: torch.Tensor,
        target_values: torch.Tensor,
    ) -> float:
        """
        1å›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—

        Args:
            states: (batch_size, 3, 8, 8)
            target_policies: (batch_size, 65)
            target_values: (batch_size, 1)

        Returns:
            float: æå¤±
        """
        self.optimizer.zero_grad()

        # AMP ã‚’ä½¿ã†å ´åˆ
        if self.scaler is not None and self.device.type == "cuda":
            with torch.amp.autocast('cuda'):
                policy_logits, value_pred = self.model(states)

                # æå¤±è¨ˆç®—
                policy_loss = self._policy_loss(policy_logits, target_policies)
                value_loss = self._value_loss(value_pred, target_values)
                total_loss = policy_loss + value_loss

            # Backward pass with scaler
            self.scaler.scale(total_loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()

        else:
            # AMP ãªã—
            policy_logits, value_pred = self.model(states)

            policy_loss = self._policy_loss(policy_logits, target_policies)
            value_loss = self._value_loss(value_pred, target_values)
            total_loss = policy_loss + value_loss

            total_loss.backward()
            self.optimizer.step()

        return total_loss.item()

    def _policy_loss(
        self,
        policy_logits: torch.Tensor,
        target_policies: torch.Tensor,
    ) -> torch.Tensor:
        """
        æ–¹ç­–æå¤±ï¼ˆã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰

        Args:
            policy_logits: (batch_size, 65) - LogSoftmaxå‡ºåŠ›
            target_policies: (batch_size, 65) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¢ºç‡åˆ†å¸ƒ

        Returns:
            torch.Tensor: æå¤±
        """
        # KL Divergence: -sum(target * log(pred))
        # policy_logits ã¯æ—¢ã« LogSoftmax ãªã®ã§ã€ç›´æ¥ä½¿ãˆã‚‹
        return -torch.mean(torch.sum(target_policies * policy_logits, dim=1))

    def _value_loss(
        self,
        value_pred: torch.Tensor,
        target_values: torch.Tensor,
    ) -> torch.Tensor:
        """
        ä¾¡å€¤æå¤±ï¼ˆMSEï¼‰

        Args:
            value_pred: (batch_size, 1)
            target_values: (batch_size, 1)

        Returns:
            torch.Tensor: æå¤±
        """
        return nn.functional.mse_loss(value_pred, target_values)

    def save_checkpoint(self, filename: str):
        """
        ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜

        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        checkpoint_path = self.checkpoint_dir / filename

        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "global_step": self.global_step,
            "epoch": self.epoch,
            "config": self.config,
        }

        torch.save(checkpoint, checkpoint_path)
        print(f"  Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """
        ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿

        Args:
            checkpoint_path: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        self.global_step = checkpoint["global_step"]
        self.epoch = checkpoint["epoch"]

        print(f"Checkpoint loaded: {checkpoint_path}")
        print(f"  Global Step: {self.global_step}")
        print(f"  Epoch: {self.epoch}")
