# オセロAIプロジェクト 外部設計書

## 1. システム全体像

本システムは、以下の2つのサブシステムで構成される。

1. **学習サブシステム (Training CLI)**
    * **ユーザー:** 開発者（あなた）
    * **目的:** AlphaZeroアルゴリズムによる自己対戦と学習を行い、最強モデルを生成する。
    * **インターフェース:** コマンドライン (Terminal)、設定ファイル。
2. **対戦サブシステム (Play GUI)**
    * **ユーザー:** プレイヤー（開発者および一般ユーザー）
    * **目的:** 学習済みモデルを用いて人間と対戦する。
    * **インターフェース:** ウィンドウアプリケーション。

---

## 2. 対戦サブシステム（GUI）の設計

ユーザーがAIと対戦するための画面設計です。Pythonの標準的なGUIライブラリ（Tkinter）またはPySide6を想定しています。

### 2.1 画面レイアウト案

ウィンドウサイズは固定（例: 800x600）とし、左側に盤面、右側に操作パネルを配置する。

```text
+-------------------------------------------------------+
|  Othello AI - AlphaZero (v1.0)                  [-][x]|
|-------------------------------------------------------|
|                                                       |
|   +-------------------+    +----------------------+   |
|   |                   |    | [ ステータス表示 ]   |   |
|   |    (A) 盤面       |    | 現在の手番: 黒 (Player)|   |
|   |                   |    | 黒: 2  白: 2         |   |
|   |      8 x 8        |    +----------------------+   |
|   |    グリッド       |                               |
|   |                   |    +----------------------+   |
|   |                   |    | [ コントロール ]     |   |
|   |                   |    | [ ニューゲーム ]     |   |
|   |                   |    | [ 待った (Undo) ]    |   |
|   |                   |    |                      |   |
|   |                   |    | AI強さ: [ Lv3 (Max)]▼|   |
|   |                   |    | 先手/後手: [ ランダム]▼|   |
|   +-------------------+    +----------------------+   |
|                                                       |
|                            +----------------------+   |
|                            | [ デバッグ情報 ]     |   |
|                            | 評価値: +0.25        |   |
|                            | 思考時間: 0.15s      |   |
|                            +----------------------+   |
|                                                       |
+-------------------------------------------------------+
```

### 2.2 画面構成要素

| エリア | 要素 | 機能・振る舞い |
| :--- | :--- | :--- |
| **(A) 盤面エリア** | オセロ盤 | 8x8の緑色のマス。黒石・白石を描画。 |
| | 合法手ガイド | 人間の手番時、置ける場所に半透明のマークを表示（クリック可能）。 |
| | 直前の手 | AIまたは人間が最後に打った場所に赤いドット等を表示して強調。 |
| **(B) ステータス** | 石カウント | 現在の黒・白の石数をリアルタイム表示。 |
| | 手番表示 | どちらの手番か、または「AI思考中...」等の状態を表示。 |
| **(C) コントロール** | ニューゲーム | 盤面をリセットし、新しい対局を開始。 |
| | 待った (Undo) | 1手（AIの手番含め2手分）戻す。 |
| | 設定プルダウン | AIモデルの選択（学習段階ごとのモデル切り替え等）。 |
| **(D) デバッグ情報** | 評価値表示 | AIが現在の盤面をどう評価しているか（-1.0〜1.0）を表示。 |

### 2.3 ユーザー操作フロー

1. **起動:** アプリを起動するとタイトル画面または初期盤面が表示される。
2. **対局開始:** 「ニューゲーム」または起動直後にゲーム開始。
3. **着手:**
    * **人間:** 合法手ガイドが表示されているマスをクリックする。置けない場所はクリック無効。
    * **AI:** 人間の着手後、自動的に思考を開始し、着手する。
4. **パス:** 置ける場所がない場合、自動的にパスのダイアログを表示し、手番を切り替える。
5. **終局:** 双方打てなくなったらゲーム終了。「黒の勝ち（34 - 30）」のようなダイアログを表示する。

---

## 3. 学習サブシステム（CLI）の設計

開発者が学習プロセスを制御するためのインターフェースです。RTX 4050 Laptopでの運用を前提に、中断・再開が容易な設計にします。

### 3.1 実行コマンド体系

基本的にはPythonスクリプトを引数付きで実行する形式とします。

* **学習実行:**
    `uv run python main.py train --config configs/8x8_config.yaml`
  * 指定した設定ファイルに基づいて学習サイクル（Self-Play -> Train -> Evaluate）を回す。
* **学習再開:**
    `uv run python main.py train --resume checkpoints/best_model.pt`
  * 中断したモデルから学習を再開する。
* **Edax対戦（評価）:**
    `uv run python main.py pit --agent_model checkpoints/best_model.pt --edax_level 10`
  * 作成したモデルとEdaxを対戦させ、勝率を出力する。

### 3.2 設定ファイル (config.yaml)

コードを変更せずにパラメータ調整できるよう、YAMLファイルで管理します。

```yaml
system:
  gpu_id: 0
  use_mixed_precision: true  # VRAM節約のため必須
  num_workers: 4             # CPUコア数依存（自己対戦用）

game:
  board_size: 8

training:
  batch_size: 256            # RTX 4050に合わせて調整
  learning_rate: 0.001
  num_simulations: 25        # MCTSのシミュレーション回数
  epochs: 10                 # 1回の更新ループで回す回数
```

### 3.3 出力データ (ログ・モデル)

学習の進捗を可視化するために以下のファイルを出力します。

1. **コンソールログ:**
    * 現在のイテレーション数
    * 自己対戦の生成速度（games/sec）
    * Loss（Policy Loss, Value Loss）
2. **モデルファイル (.pt / .onnx):**
    * `best_model.pt`: 現在の最強モデル（PyTorch形式）
    * `checkpoint_xxxx.pt`: 定期バックアップ
    * `model_final.onnx`: GUI用エクスポートモデル
3. **TensorBoardログ:**
    * Lossの推移グラフ
    * 勝率の推移グラフ

---

## 4. データフロー設計

システム全体でのデータの流れを定義します。

1. **入力:**
    * 学習時は「自己対戦生成データ（盤面状態・確率・勝敗）」が入力となる。
2. **処理:**
    * **Trainer:** データをバッチ処理し、ResNetの重みを更新。
    * **Evaluator:** 新旧モデルを戦わせ、勝者を決定。
3. **出力:**
    * 学習済みモデル（`.pt`）が生成される。
4. **変換:**
    * `.pt` ファイルを `.onnx` に変換する（コンバータスクリプトを使用）。
5. **利用:**
    * GUIアプリが `.onnx` を読み込み、推論エンジン（ONNX Runtime）で実行する。
