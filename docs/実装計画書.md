# オセロAI開発 実装計画書

## 1. 全体スケジュール概要

* **開発期間:** 約6〜8週間（想定）
* **開発者:** 1名
* **目標:** AlphaZeroアルゴリズムによる超人的オセロAIの構築とGUIアプリ化

| フェーズ | 期間 (目安) | 主なタスク | 完了マイルストーン |
| :--- | :--- | :--- | :--- |
| **Phase 1** | 1〜1.5週間 | Cython環境構築、ビットボード実装 | 1万局/秒の対戦速度達成 |
| **Phase 2** | 2〜3週間 | NN実装、MCTS、学習ループ構築 | 学習サイクルがエラーなく回る |
| **Phase 3** | 2〜4週間 | 本格学習、Edaxベンチマーク | Edax Lv10に勝利 |
| **Phase 4** | 1〜2週間 | ONNX化、GUIアプリ実装 | 配布可能なアプリの完成 |

---

## 2. 詳細タスクリスト

### Phase 1: コア・ロジックと高速化 (Core Logic Optimization)

**目的:** Pythonのボトルネックを排除し、強化学習に必要な膨大なシミュレーション速度を確保する。

#### 1.1 プロジェクト環境構築 (Day 1)

* [ ] GitHubリポジトリ作成 (`othello-alphazero`)
* [ ] ディレクトリ構成の作成（`src/cython`, `src/model` 等）
* [ ] 仮想環境作成 (conda/venv) と `requirements.txt` 定義
  * `numpy`, `torch`, `cython`, `pytest`

#### 1.2 Cythonビットボード実装 (Day 2-5)

* [ ] `src/cython/bitboard.pxd` (ヘッダ) 定義
* [ ] `src/cython/bitboard.pyx` 実装
  * [ ] `reset()`: 初期配置設定
  * [ ] `get_legal_moves()`: ビットシフト演算による合法手生成
  * [ ] `make_move()`: 反転処理の実装
  * [ ] `is_terminal()`: 終局判定
* [ ] `setup.py` 作成とビルド確認 (`uv run python setup.py build_ext --inplace`)

#### 1.3 動作テストとベンチマーク (Day 6-7)

* [ ] `tests/test_bitboard.py`: 既知の局面を与えて正しい合法手が出るかテスト（単体テスト）
* [ ] `benchmark.py`: ランダムプレイヤー同士で対戦させ、FPS (Games Per Second) を計測
  * **目標:** 1スレッドあたり 5,000〜10,000 games/sec 以上

---

### Phase 2: 学習システムの構築 (System Implementation)

**目的:** AlphaZeroのアルゴリズム（Self-Play, Train, Evaluate）を実装し、学習が開始できる状態にする。

#### 2.1 ニューラルネットワーク実装 (Day 8-10)

* [ ] `src/model/net.py`: PyTorchで `OthelloResNet` を実装
  * [ ] `ResBlock`: 残差ブロック定義
  * [ ] `PolicyHead` / `ValueHead`: 出力層定義
* [ ] RTX 4050での動作確認
  * [ ] ダミー入力 `(Batch, 3, 8, 8)` を流し、GPUメモリ使用量と推論速度を確認

#### 2.2 MCTS (モンテカルロ木探索) 実装 (Day 11-14)

* [ ] `src/mcts/node.py`: 木構造のノードクラス定義
* [ ] `src/mcts/mcts.py`: 探索ロジック実装
  * [ ] `PUCT` アルゴリズムの実装
  * [ ] **重要:** `Action Masking` (合法手以外の確率ゼロ化) の実装
  * [ ] ルートノードへのディリクレノイズ付加

#### 2.3 学習ループの実装 (Day 15-18)

* [ ] `src/train/self_play.py`: 自己対戦ワーカーの実装
  * [ ] データの保存形式 (`state`, `policy`, `value`) の定義
* [ ] `src/train/trainer.py`: 学習ロジックの実装
  * [ ] リプレイバッファの実装
  * [ ] `AMP (Automatic Mixed Precision)` の適用（VRAM節約）
* [ ] パイプライン結合テスト:
  * Self-Play 1回 -> Train 1 epoch -> Model Update が通るか確認

---

### Phase 3: 本格学習とチューニング (Training & Evaluation)

**目的:** モデルを実際に賢くし、客観的な強さを証明する。

#### 3.1 小規模実験 (6x6盤面 / 軽量モデル) (Day 19-21)

* [ ] 6x6オセロ設定での学習実施（数時間〜半日）
* [ ] 正常にLossが下がり、ランダムプレイヤーに勝てるようになるか確認（サニティチェック）

#### 3.2 8x8 本格学習開始 (Day 22〜継続)

* [ ] パラメータ設定: Block数10, Filter128, Sim数25〜50
* [ ] 学習実行（RTX 4050にて夜間/バックグラウンド実行）
* [ ] TensorBoardによる監視（Loss, 勝率, ゲーム長）

#### 3.3 評価システム構築 (Day 25-27)

* [ ] `src/eval/edax_wrapper.py`: Edaxをサブプロセスで呼ぶラッパー実装
* [ ] 対戦スクリプト作成: 自作モデル vs Edax (Level 1から順に)
* [ ] ベンチマーク実施: 定期的にEdaxと戦わせ、進捗をグラフ化

---

### Phase 4: アプリケーション化 (Application Deployment)

**目的:** 一般ユーザーが遊べる形にパッケージングする。

#### 4.1 モデル軽量化 (Day 35-37)

* [ ] 学習済みモデル (`.pt`) を ONNX形式 (`.onnx`) にエクスポート
* [ ] `onnxruntime` を用いた推論クラスの実装と速度計測

#### 4.2 GUI実装 (Day 38-42)

* [ ] フレームワーク選定 (Tkinter or PySide6)
* [ ] 盤面描画クラスの実装 (クリックイベント、石の描画)
* [ ] ゲーム進行スレッドの分離 (UIフリーズ防止)
* [ ] 機能実装: 「待った」「ヒント（評価値表示）」「AIレベル選択」

#### 4.3 最終調整と配布 (Day 43-45)

* [ ] バグ出し・UI調整
* [ ] (任意) `PyInstaller` 等での実行ファイル化

---

## 3. リスク管理と対策

| リスク | 対策案 |
| :--- | :--- |
| **VRAM不足 (OOM)** | バッチサイズを減らす (256 -> 128)。<br>モデルサイズ縮小 (Filter 128 -> 64)。<br>混合精度学習 (AMP) は必須。 |
| **学習が収束しない** | ハイパーパラメータ (LR, PUCT係数) の見直し。<br>まずは6x6盤面で確実にバグがないか検証する。 |
| **Cythonが遅い** | プロファイラ (`cProfile`, `line_profiler`) でボトルネック特定。<br>Pythonオブジェクトの生成を極力減らす。 |
| **Edaxに勝てない** | MCTSのシミュレーション数を増やす（思考時間を伸ばす）。<br>学習データ生成時のノイズ探索を調整し、多様な局面を学習させる。 |

---

## 4. 推奨される次のアクション

まずは **Phase 1: 環境構築とCython実装** に着手します。
以下の手順で進めてください。

1. 作業用フォルダを作成する。
2. Python仮想環境を用意する。
3. **Cythonでビットボードのプロトタイプを作成する（ここからコード提供可能）。**
